{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Selvaria\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# -*-conding: utf8 -*-\n",
    "# !/usr/bin/python\n",
    "# Author: Selvaria\n",
    "\n",
    "# 残差网络\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 6, 6, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#残差块的实现如下。它可以设定输出通道数、是否使用额外的1×1卷积层来修改通道数以及卷积层的步幅。\n",
    "\n",
    "from tensorflow.keras import layers,activations\n",
    "\n",
    "class Residual(tf.keras.Model):\n",
    "    def __init__(self, num_channels, input_shape, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(2,\n",
    "                                   padding='same',\n",
    "                                   kernel_size=3,\n",
    "                                   strides=strides,\n",
    "                                  input_shape=input_shape)\n",
    "        self.conv2 = layers.Conv2D(num_channels, kernel_size=3,padding='same')\n",
    "        if use_1x1conv: #使用额外的1×1卷积层来修改(输出的)通道数以及卷积层的步幅\n",
    "            self.conv3 = layers.Conv2D(num_channels,\n",
    "                                       kernel_size=(1,1),\n",
    "                                       strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, X):\n",
    "        Y = activations.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return activations.relu(Y + X)\n",
    "\n",
    "# 此时输入和输出形状一致\n",
    "blk = Residual(3,(6, 6, 3))\n",
    "X = tf.random.uniform((4, 6, 6, 3))\n",
    "blk(X).shape#TensorShape([4, 6, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_77 (Conv2D)           (None, 6, 6, 3)           84        \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 6, 6, 3)           84        \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 6, 6, 3)           12        \n",
      "=================================================================\n",
      "Total params: 180\n",
      "Trainable params: 174\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=3,kernel_size=3, padding='same', strides=1, input_shape=(6, 6, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=3,kernel_size=3, padding='same', activation='relu'))\n",
    "# 这里其实还有一层是将输入和现在的层加了一下，只不过不影响输出形状，可以通过自定义层实现，这里就不写了\n",
    "# model.layer = model.output + model.input\n",
    "# model.add(model.layer)\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 6, 6, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 3, 3, 6)      168         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 3, 3, 6)      24          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 3, 3, 6)      330         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 3, 3, 6)      0           conv2d_72[0][0]                  \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 3, 3, 6)      24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 3, 3, 6)      24          batch_normalization_41[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 570\n",
      "Trainable params: 546\n",
      "Non-trainable params: 24\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate,add\n",
    "\n",
    "inputs = Input((6, 6, 3))\n",
    "\n",
    "c1 = Conv2D(6, (3, 3), activation='relu', padding='same', strides=2) (inputs)\n",
    "c2 = Conv2D(6, (3, 3), activation='relu', padding='same') (c1)\n",
    "\n",
    "u1 = Conv2D(6, (1, 1), strides=2, padding='same') (inputs) \n",
    "# u2 = concatenate([u1, c2])\n",
    "u2 = add([u1, c2])\n",
    "\n",
    "b1 = tf.keras.layers.BatchNormalization()(u2)\n",
    "\n",
    "outputs = tf.keras.layers.BatchNormalization()(b1)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 3, 3, 6])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时输出形状长宽减半（靠步幅。正常情况是靠卷积核），输出通道数量也有调整\n",
    "blk = Residual(6, (6, 6, 3),use_1x1conv=True, strides=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_140 (Conv2D)          (None, 112, 112, 64)      3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 56, 56, 64)        0         \n",
      "=================================================================\n",
      "Total params: 3,456\n",
      "Trainable params: 3,328\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ResNet的前两层：\n",
    "\n",
    "net = tf.keras.models.Sequential(\n",
    "    [layers.Conv2D(64, kernel_size=7, strides=2, padding='same', input_shape=(224, 224, 1), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=3, strides=2, padding='same')])\n",
    "\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之后是残差块，每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "# 注意，这里对第一个模块做了特别处理。\n",
    "\n",
    "class ResnetBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_channels, num_residuals, first_block=False,**kwargs):\n",
    "        super(ResnetBlock, self).__init__(**kwargs)\n",
    "        self.listLayers=[]\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block: # #第一层不需减半长宽，使用else的层结构\n",
    "                self.listLayers.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                self.listLayers.append(Residual(num_channels))\n",
    "                \n",
    "    def call(self, X):\n",
    "        for layer in self.listLayers.layers:\n",
    "            X = layer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "first_block = True\n",
    "i == 0 and not first_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接着我们为ResNet加入所有残差块。这里每个模块使用两个残差块。\n",
    "\n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self,num_blocks,i_shape,**kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.conv=layers.Conv2D(64, kernel_size=7, strides=2, padding='same', input_shape=i_shape, activation='relu')\n",
    "        self.bn=layers.BatchNormalization()\n",
    "#         self.relu=layers.Activation('relu')\n",
    "        self.mp=layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "        self.resnet_block1=ResnetBlock(64,num_blocks[0], first_block=True)\n",
    "        self.resnet_block2=ResnetBlock(128,num_blocks[1])\n",
    "        self.resnet_block3=ResnetBlock(256,num_blocks[2])\n",
    "        self.resnet_block4=ResnetBlock(512,num_blocks[3])\n",
    "        self.gap=layers.GlobalAvgPool2D()\n",
    "        self.fc=layers.Dense(units=10,activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x=self.conv(x)\n",
    "        x=self.bn(x)\n",
    "#         x=self.relu(x)\n",
    "        x=self.mp(x)\n",
    "        x=self.resnet_block1(x)\n",
    "        x=self.resnet_block2(x)\n",
    "        x=self.resnet_block3(x)\n",
    "        x=self.resnet_block4(x)\n",
    "        x=self.gap(x)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "mynet=ResNet([2,2,2,2],i_shape=(224, 224, 1)) # 这里每个模块使用两个残差块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_99 (Conv2D)           (None, 112, 112, 64)      3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "resnet_block_16 (ResnetBlock (None, 56, 56, 64)        148736    \n",
      "_________________________________________________________________\n",
      "resnet_block_17 (ResnetBlock (None, 28, 28, 128)       526976    \n",
      "_________________________________________________________________\n",
      "resnet_block_18 (ResnetBlock (None, 14, 14, 256)       2102528   \n",
      "_________________________________________________________________\n",
      "resnet_block_19 (ResnetBlock (None, 7, 7, 512)         8399360   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 11,186,186\n",
      "Trainable params: 11,178,378\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mynet= tf.keras.models.Sequential()\n",
    "mynet.add(layers.Conv2D(64, kernel_size=7, strides=2, padding='same', input_shape=(224, 224, 1), activation='relu'))\n",
    "mynet.add(layers.BatchNormalization())\n",
    "mynet.add(layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n",
    "mynet.add(ResnetBlock(64,2, first_block=True))\n",
    "mynet.add(ResnetBlock(128,2))\n",
    "mynet.add(ResnetBlock(256,2))\n",
    "mynet.add(ResnetBlock(512,2))\n",
    "mynet.add(layers.GlobalAvgPool2D())\n",
    "mynet.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "mynet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_119 (Conv2D)          (None, 14, 14, 64)        3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "resnet_block_20 (ResnetBlock (None, 7, 7, 64)          148736    \n",
      "_________________________________________________________________\n",
      "resnet_block_21 (ResnetBlock (None, 4, 4, 128)         526976    \n",
      "_________________________________________________________________\n",
      "resnet_block_22 (ResnetBlock (None, 2, 2, 256)         2102528   \n",
      "_________________________________________________________________\n",
      "resnet_block_23 (ResnetBlock (None, 1, 1, 512)         8399360   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 11,186,186\n",
      "Trainable params: 11,178,378\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mynet= tf.keras.models.Sequential()\n",
    "mynet.add(layers.Conv2D(64, kernel_size=7, strides=2, padding='same', input_shape=(28, 28, 1), activation='relu'))\n",
    "mynet.add(layers.BatchNormalization())\n",
    "mynet.add(layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n",
    "mynet.add(ResnetBlock(64,2, first_block=True))\n",
    "mynet.add(ResnetBlock(128,2))\n",
    "mynet.add(ResnetBlock(256,2))\n",
    "mynet.add(ResnetBlock(512,2))\n",
    "mynet.add(layers.GlobalAvgPool2D())\n",
    "mynet.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "mynet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 15s 319us/sample - loss: 0.4729 - accuracy: 0.8332 - val_loss: 0.5447 - val_accuracy: 0.8236\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 13s 263us/sample - loss: 0.3160 - accuracy: 0.8846 - val_loss: 0.3569 - val_accuracy: 0.8773\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 13s 266us/sample - loss: 0.2774 - accuracy: 0.8976 - val_loss: 0.4318 - val_accuracy: 0.8577\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 13s 267us/sample - loss: 0.2519 - accuracy: 0.9068 - val_loss: 0.3663 - val_accuracy: 0.8685\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 13s 265us/sample - loss: 0.2329 - accuracy: 0.9130 - val_loss: 0.2889 - val_accuracy: 0.9036\n",
      "10000/10000 - 2s - loss: 0.2979 - accuracy: 0.9007\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "mynet.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = mynet.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.2)\n",
    "test_scores = mynet.evaluate(x_test, y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
